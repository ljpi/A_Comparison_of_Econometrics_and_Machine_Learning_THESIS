---
title: "Machine Learning for Time Series: Predicting a Recession"
author: "Lester Pi"
date: "June 10, 2017"
output: pdf_document
---

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Introduction

This projects compares traditional time series forecasting methods with machine learning techniques. Since this is an offshoot of my capstone project, I will cover the testing methods and conclusions of my capstone in brief and build upon them. Skip to the "Project Extension" section if you would like to skip over the capstone work.

#Capstone Work

The premise of the capstone was to compare ARIMA with machine learning, more explicitly LASSO, decision trees, and neural networks, in a time series framework. The time series of choice is the volatility of the S&P 500. For machine learning, I introduced new variables into the data set including presidential approval ratings, interest rate, and others. The machine learning techniques surpassed the ARIMA model when comparing the MAPE (recursive 2.91353, rolling 2.93426) with a tuned neural netowrk performing with the lowest MAPE (2.2889). I concluded that the machine learning techniques, especially neural networks, can effectively take the place of ARIMA for time series forecasting.

The following code has had its output surpressed until the "Project Extension" section.

```{r, warning=FALSE, message=FALSE}
library('xts')
library("quantmod")
library('forecast')
library('dynlm')
library('vars')
library('tseries')
library('glmnet')
library('randomForest')
library('neuralnet')
library('plyr') 
library('glarma')
library('caret')
```

```{r, warning=FALSE, , message=FALSE}
setwd("C:/cygwin64/home/Lester/thesis")
```


```{r, warning=FALSE, , message=FALSE}
#define functions
DateToInt = function(d){
  switch(d,January={return(1)},February={return(2)},March={return(3)},
         April={return(4)},May={return(5)},June={return(6)},
         July={return(7)},August={return(8)},September={return(9)},
         October={return(10)},November={return(11)},December={return(12)})
  return(NA)
}

IntToDate = function(i){
  switch(i,"1"={return("January")},"2"={return("February")},"3"={return("March")},
         "4"={return("April")},"5"={return("May")},"6"={return("June")},
         "7"={return("July")},"8"={return("August")},"9"={return("September")},
         "10"={return("October")},"11"={return("November")},"12"={return("December")})
  return(NA)
}

mape = function(y, yhat){
  return(mean(abs(y - yhat)/abs(y)*100))
}


backtest = function(ts, step_size, type){
  results = c()
  index = floor(2*length(ts)/3)


    y = c()
    y_hat = c()


  if(type == "recursive"){
    
    while(index < length(ts)-step_size){
      temp_mod = auto.arima(ts[1:index])
      temp_forecast = forecast(temp_mod,h=step_size)
      start = index+1
      end = index+step_size
      # results=c(results,mape(ts[start:end],temp_forecast$mean))
      
      y=c(y,ts[(index+1):(index+step_size)])
      y_hat=c(y_hat,temp_forecast$mean)
      
      index = index+1
    }
  }
  
  else if(type == "rolling"){
    count=0
    while(index < length(ts)-step_size){
      temp_mod = auto.arima(ts[(1+count):index]) 
      temp_forecast = forecast(temp_mod,h=step_size)
      # results=c(results,mape(ts[(index+1):(index+step_size)],temp_forecast$mean))
      y=c(y,ts[(index+1):(index+step_size)])
      y_hat=c(y_hat,temp_forecast$mean)

      index=index+1
      count=count+1
    }
  }
    #   print(y)
    # print(y_hat)
  results = list(y,y_hat)
  return(results)
}


```


```{r, warning=FALSE, , message=FALSE}
options(scipen=999)


VIX = read.csv("^VIX.csv",stringsAsFactors=FALSE)
rownames(VIX)=as.Date(VIX$Date)
vix = VIX$Adj.Close
names(vix) = as.Date(VIX$Date)
vix = na.omit(vix)


GSPC = read.csv("^SP500TR.csv",stringsAsFactors=FALSE)
rownames(GSPC)=as.Date(GSPC$Date)
sp500 = GSPC$Adj.Close
names(sp500) = as.Date(GSPC$Date)
sp500 = na.omit(sp500)
print(length(vix))
print(length(sp500))
```


```{r, warning=FALSE, , message=FALSE}

print(length(sp500))
print(length(vix))

#transform into returns
sp500_returns = na.omit(diff(sp500)/sp500[-length(sp500)])

window_size = 30
volatility_sp500 = na.omit(volatility(sp500[1:length(sp500)], n=window_size))

pres_approval = read.csv("president_approval.csv",stringsAsFactors = FALSE)

pres_approval$republican = ifelse(pres_approval$President_Name=="Donald J. Trump"|
                               pres_approval$President_Name=="George W. Bush"|
                               pres_approval$President_Name=="George H.W. Bush",1,0)
pres_approval$End_Date = as.Date(pres_approval$End_Date,"%m/%d/%y")

pres_average = pres_approval

pres_average$Month <- months(pres_approval$End_Date)


pres_average$Year <- format(pres_approval$End_Date,format="%Y")


approval_average = aggregate( Approval ~ Month + Year,pres_average , mean )
disaproval_average = aggregate( Disapproval ~ Month + Year,pres_average , mean )
unknown_average = aggregate( Unsure.No_Data ~ Month + Year,pres_average , mean )

```



```{r, warning=FALSE, , message=FALSE}
#make same length as volatility
vix_volatility=vix[30:length(vix)]


plot(volatility_sp500*100,col="red",type='l')
lines(vix_volatility)

ts_vix = ts(vix_volatility)
ts_vol = ts(volatility_sp500)

adf.test(ts_vol)

#benchmark
recursive=(backtest(ts_vol,1,"recursive"))
rolling=(backtest(ts_vol,1,"rolling"))
mape(recursive[[1]],recursive[[2]])
mape(rolling[[1]],rolling[[2]])



```


```{r, warning=FALSE, , message=FALSE}

vix_mape=c()
for(i in 1:length(vix_volatility)){
  vix_mape=c(vix_mape,mape(volatility_sp500[i]*100,vix_volatility[i]))
}
plot(1:length(vix_mape),vix_mape,main="Vix Volatility Mape",xlab="Prediction Index")
abline(h=mean(vix_mape),col='red')

print(mean(na.omit(vix_mape)))


```


```{r, warning=FALSE, , message=FALSE}

vol_df = data.frame(volatility_sp500)
vol_df$vix = vix_volatility

```


```{r, warning=FALSE, , message=FALSE}

#output file csv
filename = "out.csv"

if(file.exists(filename)){
  file.remove(filename)
}
file.create(filename)
# outfile = file(filename)

#construct nn input data

names(volatility_sp500) = names(sp500[30:length(sp500)])

col_names = "target"

for(i in 30:length(volatility_sp500)){

  outString = ""
  for(j in 0:(window_size-2)){
    outString = paste(outString, volatility_sp500[i-j-1], sp500[i+j+1], vix[i+j], sp500_returns[i+j] ,sep=",")
    if(i==30){
      col_names = paste(col_names, paste(",volatilityL(",(j+1),")",sep=""), paste(",sp500L(",(i-(j+1)),")",sep=""), 
                    paste(",vixL(",(i-(j+1)),")",sep=""), paste(",sp500returnsL(",(i-(j+1)),")",sep=""),sep = "")
    }
  }

  #get presidential info
  month_string = months(as.Date(names(volatility_sp500[i])))
  month_int = DateToInt(month_string)
  year = format(as.Date(names(volatility_sp500[i])),format="%Y")

  #current month's avg approval rating
  approval_avg = subset(approval_average$Approval,approval_average$Month==month_string&approval_average$Year==year)
  disapproval_avg = subset(disaproval_average$Disapproval,disaproval_average$Month==month_string&disaproval_average$Year==year)
  unknown_avg = subset(unknown_average$Unsure.No_Data,unknown_average$Month==month_string&unknown_average$Year==year)

  #move back a month
  if(length(approval_avg)==0){
    tempM = month_int-1
    tempY = year
    if(tempM<0){
      tempM=12
      tempY=tempY-1
    }
    tempM_string = IntToDate(tempM)
    approval_avg = subset(approval_average$Approval,approval_average$Month==tempM_string&approval_average$Year==tempY)
    disapproval_avg = subset(disaproval_average$Disapproval,disaproval_average$Month==tempM_string&disaproval_average$Year==tempY)
    unknown_avg = subset(unknown_average$Unsure.No_Data,unknown_average$Month==tempM_string&unknown_average$Year==tempY)
  }

  if(i==30){
    col_names=paste(col_names,",pres_approv_avg,pres_disapprov_avg,pres_unknown_avg",sep="")
  }

  outString = paste(outString,approval_avg,disapproval_avg,unknown_avg,sep=",")

  #remove first comma
  outString = substring(outString,2,nchar(outString))
  #add on output
  outString = paste(volatility_sp500[i],outString,sep=",")

  #remove first comma
  outString = substring(outString,2,nchar(outString))

  #write to outfile
  if(i==30){
    cat(col_names,file=filename,append=TRUE,sep="\n")
  }
  cat(outString,file=filename,append=TRUE,sep="\n")

}


```


```{r, warning=FALSE, , message=FALSE}

full_data = read.csv("out.csv",header = TRUE)
rownames(full_data) = names(volatility_sp500[30:length(volatility_sp500)])
# head(full_data)


#randomized vs timeseries?
#create training and test sets
## 66% of the sample size
smp_size <- floor(.66* nrow(full_data))

## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(full_data)), size = smp_size)

train <- full_data[train_ind, ]
test <- full_data[-train_ind, ]


#lasso
x <- model.matrix( ~ .-1, train[ , -1])
y <- data.matrix(train[, 1])

model.lasso <- cv.glmnet(x, y, family='gaussian', alpha=1, parallel=TRUE, standardize=TRUE)
plot(model.lasso)
plot(model.lasso$glmnet.fit, xvar="lambda", label=TRUE)
model.lasso$lambda.min
model.lasso$lambda.1se
coef(model.lasso, s=model.lasso$lambda.min)

#decision tree
tree_fit <- randomForest(target ~ .,   data=train)
print(tree_fit) # view results 
importance(tree_fit) # importance of each predictor
varImpPlot(tree_fit, main = "Importance Plot", n.var = 15)

```


```{r, warning=FALSE, , message=FALSE}

test_x = test[,-1]
test_x_matrix = model.matrix( ~ .-1, test[,-1])
lasso_test = predict(model.lasso, newx=test_x_matrix,type="link")
tree_test = predict(tree_fit, newdata=test_x)


#mape
lasso_mape = mape(test$target,lasso_test)
lasso_mape

tree_mape = mape(test$target,tree_test)
tree_mape


```


```{r, warning=FALSE, , message=FALSE}

set.seed(1)

#normalize data

maxs <- apply(full_data, 2, max) 
mins <- apply(full_data, 2, min)

scaled <- as.data.frame(scale(full_data, center = mins, scale = maxs - mins))


train_ <- scaled[train_ind,]
test_ <- scaled[-train_ind,]


n <- names(train_)
f <- as.formula(paste("target ~", paste(n[!n %in% "target"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(100,70,60,50,40,30,20),linear.output=T)

pr.nn <- compute(nn,test_[,2:ncol(test_)])


pr.nn_ <- pr.nn$net.result*(max(full_data$target)-min(full_data$target))+min(full_data$target)
test.r <- (test_$target)*(max(full_data$target)-min(full_data$target))+min(full_data$target)

MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)


mape_nn = mape(test.r,pr.nn_)
mape_nn

```


```{r, warning=FALSE, , message=FALSE}
#load and massage data
recession = read.csv("USRECD.csv")
interest = read.csv("DFF.csv")
rownames(recession) = as.Date(recession$DATE)
recession = recession[,2, drop=FALSE]
rownames(interest) = as.Date(interest$DATE)
interest = interest[,2, drop=FALSE]


#momentums
interest_momentum = apply( interest , 2 , diff )
head(interest_momentum)


full_data_update <- merge(full_data, recession, by=0, all=TRUE) 
full_data_update = full_data_update[ , !(names(full_data_update) %in% c("Row.names"))]
full_data_update = na.omit(full_data_update)
rownames(full_data_update) = names(volatility_sp500[30:length(volatility_sp500)])
full_data_update <- merge(full_data_update, interest, by=0, all=TRUE) 
full_data_update = full_data_update[ , !(names(full_data_update) %in% c("Row.names"))]
full_data_update = na.omit(full_data_update)
rownames(full_data_update) = names(volatility_sp500[30:length(volatility_sp500)])
full_data_update <- merge(full_data_update, interest_momentum, by=0, all=TRUE) 
full_data_update = full_data_update[ , !(names(full_data_update) %in% c("Row.names"))]
full_data_update = na.omit(full_data_update)
rownames(full_data_update) = names(volatility_sp500[30:length(volatility_sp500)])
head(full_data_update)

```


```{r, warning=FALSE, , message=FALSE}

#create training and test sets
## 66% of the sample size
smp_size <- floor(.66* nrow(full_data_update))

## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(full_data_update)), size = smp_size)

train <- full_data_update[train_ind, ]
test <- full_data_update[-train_ind, ]


maxs <- apply(full_data_update, 2, max) 
mins <- apply(full_data_update, 2, min)

scaled <- as.data.frame(scale(full_data_update, center = mins, scale = maxs - mins))


train_ <- scaled[train_ind,]
test_ <- scaled[-train_ind,]

```


```{r, warning=FALSE, , message=FALSE}
#lasso
x <- model.matrix( ~ .-1, train[ , -1])
y <- data.matrix(train[, 1])

model.lasso <- cv.glmnet(x, y, family='gaussian', alpha=1, parallel=TRUE, standardize=TRUE)
plot(model.lasso)
plot(model.lasso$glmnet.fit, xvar="lambda", label=TRUE)
model.lasso$lambda.min
model.lasso$lambda.1se
coef(model.lasso, s=model.lasso$lambda.min)

#decision tree
tree_fit <- randomForest(target ~ .,   data=train)
print(tree_fit) # view results 
importance(tree_fit) # importance of each predictor
varImpPlot(tree_fit, n.var = 15)

#nn
set.seed(1)

n <- names(train_)
f <- as.formula(paste("target ~", paste(n[!n %in% "target"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(100,70,60,50,40,30,20),linear.output=T)

```


```{r, warning=FALSE, , message=FALSE}
test_x = test[,-1]
test_x_matrix = model.matrix( ~ .-1, test[,-1])
lasso_test = predict(model.lasso, newx=test_x_matrix,type="link")
tree_test = predict(tree_fit, newdata=test_x)

lasso_mape = mape(test$target,lasso_test)
lasso_mape

tree_mape = mape(test$target,tree_test)
tree_mape


#nn
pr.nn <- compute(nn,test_[,2:ncol(test_)])
# pr.nn
pr.nn_ <- pr.nn$net.result*(max(full_data$target)-min(full_data$target))+min(full_data$target)
test.r <- (test_$target)*(max(full_data$target)-min(full_data$target))+min(full_data$target)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
# MSE.nn
mape_nn = mape(test.r,pr.nn_)
mape_nn

```


```{r, warning=FALSE, , message=FALSE}
#extract non 0s from lasso
coefs = coef(model.lasso, s=model.lasso$lambda.min)

non_0_coefs=c()
for( i in 2: length(coefs) ){
  if(coefs[i]!=0){
    non_0_coefs = c(non_0_coefs,rownames(coefs)[i])
  }
}

#add in target
non_0_coefs = c("target",non_0_coefs)
```


```{r, warning=FALSE, , message=FALSE}
# recreate training and test sets
full_data_minimized = full_data_update[ , which(names(full_data_update) %in% non_0_coefs)]

#randomized vs timeseries?
#create training and test sets
## 66% of the sample size
smp_size <- floor(.66* nrow(full_data_minimized))

## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(full_data_minimized)), size = smp_size)

train <- full_data_minimized[train_ind, ]
test <- full_data_minimized[-train_ind, ]

#normalize data for nn

maxs <- apply(full_data_minimized, 2, max)
mins <- apply(full_data_minimized, 2, min)

scaled <- as.data.frame(scale(full_data_minimized, center = mins, scale = maxs - mins))


train_ <- scaled[train_ind,]
test_ <- scaled[-train_ind,]
```


```{r, warning=FALSE, , message=FALSE}
#lasso
x <- model.matrix( ~ .-1, train[ , -1])
y <- data.matrix(train[, 1])

model.lasso <- cv.glmnet(x, y, family='gaussian', alpha=1, parallel=TRUE, standardize=TRUE)
plot(model.lasso)
plot(model.lasso$glmnet.fit, xvar="lambda", label=TRUE)
model.lasso$lambda.min
model.lasso$lambda.1se
coef(model.lasso, s=model.lasso$lambda.min)

#decision tree
tree_fit <- randomForest(target ~ .,   data=train)
print(tree_fit) # view results
importance(tree_fit) # importance of each predictor
varImpPlot(tree_fit)

#nn
set.seed(1)

n <- names(train_)
f <- as.formula(paste("target ~", paste(n[!n %in% "target"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(100,70,60,50,40,30,20),linear.output=T)
```


```{r, warning=FALSE, , message=FALSE}
test_x = test[,-1]
test_x_matrix = model.matrix( ~ .-1, test[,-1])
lasso_test = predict(model.lasso, newx=test_x_matrix,type="link")
tree_test = predict(tree_fit, newdata=test_x)

lasso_mape = mape(test$target,lasso_test)
lasso_mape

tree_mape = mape(test$target,tree_test)
tree_mape


#nn
pr.nn <- compute(nn,test_[,2:ncol(test_)])
# pr.nn
pr.nn_ <- pr.nn$net.result*(max(full_data$target)-min(full_data$target))+min(full_data$target)
test.r <- (test_$target)*(max(full_data$target)-min(full_data$target))+min(full_data$target)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
# MSE.nn
mape_nn = mape(test.r,pr.nn_)
mape_nn
```


```{r, warning=FALSE, , message=FALSE}

set.seed(1)
nn <- neuralnet(f,data=train_,hidden=c(30,15,6),linear.output=T)
pr.nn <- compute(nn,test_[,2:ncol(test_)])
# pr.nn
pr.nn_ <- pr.nn$net.result*(max(full_data$target)-min(full_data$target))+min(full_data$target)
test.r <- (test_$target)*(max(full_data$target)-min(full_data$target))+min(full_data$target)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
# MSE.nn
mape_nn = mape(test.r,pr.nn_)
mape_nn


```


```{r, warning=FALSE, , message=FALSE}

nn_func=function(nodes){
  set.seed(1)
  nn <- neuralnet(f,data=train_,hidden=nodes,linear.output=T)
  pr.nn <- compute(nn,test_[,2:ncol(test_)])
  # pr.nn
  pr.nn_ <- pr.nn$net.result*(max(full_data$target)-min(full_data$target))+min(full_data$target)
  test.r <- (test_$target)*(max(full_data$target)-min(full_data$target))+min(full_data$target)
  MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
  # MSE.nn
  mape_nn = mape(test.r,pr.nn_)
  return(mape_nn)
}

##comment out when not in use

# iterations = 10
# #randomly select values around 30, 15, 6
# for(i in 1:iterations){
#
#   x1 <- floor(runif(1, 15, 25))
#   x2 <- floor(runif(1, 10, 16))
#   x3 <- floor(runif(1, -2, 3))
#
#   node_list=c()
#
#   if(x3>0){
#     node_list = c(x1,x2,x3)
#   }
#   else{
#     node_list = c(x1,x2)
#   }
#   mape1 = nn_func(node_list)
#   if(mape1<=2.24627944){
#     print(node_list)
#     print(mape1)
#   }
#
# }

```


```{r, warning=FALSE, , message=FALSE}

set.seed(1)
nn <- neuralnet(f,data=train_,hidden=c(20,13,1),linear.output=T)
pr.nn <- compute(nn,test_[,2:ncol(test_)])
# pr.nn
pr.nn_ <- pr.nn$net.result*(max(full_data$target)-min(full_data$target))+min(full_data$target)
test.r <- (test_$target)*(max(full_data$target)-min(full_data$target))+min(full_data$target)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
# MSE.nn
mape_nn = mape(test.r,pr.nn_)
print(mape_nn)
plot(x=as.Date(rownames(pr.nn_)),y=pr.nn_, type='p',col="blue", ylab="Volatility",xlab="Date",main="ANN Predicted VS Actual")
lines(x=as.Date(rownames(pr.nn_)),test.r,col="red")
legend("topleft",c("Predicted","Actual"),lty=c(0,1), col = c('blue','red'), pch=c(1,NA))



```


```{r, warning=FALSE, , message=FALSE}

#normalize data

maxs <- apply(full_data_minimized, 2, max)
mins <- apply(full_data_minimized, 2, min)

scaled <- as.data.frame(scale(full_data_minimized, center = mins, scale = maxs - mins))

set.seed(34)

cv.error <- NULL
mape_nn_cv = NULL
k <- 5


pbar <- create_progress_bar('text')
pbar$init(k)

for(i in 1:k){
    index <- sample(1:nrow(scaled),round(0.8*nrow(scaled)))
    train.cv <- scaled[index,]
    test.cv <- scaled[-index,]

    nn <- neuralnet(f,data=train.cv,hidden=c(20,13,1),linear.output=T)

    pr.nn <- compute(nn,test.cv[,2:ncol(test.cv)])
    pr.nn <- pr.nn$net.result*(max(full_data$target)-min(full_data$target))+min(full_data$target)

    test.cv.r <- (test.cv$target)*(max(full_data$target)-min(full_data$target))+min(full_data$target)

    cv.error[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv)

    mape_nn_cv[i] = mape(test.cv.r,pr.nn)


    pbar$step()
}

mean(cv.error)
mean(mape_nn_cv)


boxplot(mape_nn_cv,xlab='MAPE CV',col='cyan',
        border='blue',names='CV error (MAPE)',
        main='CV K-Fold (5) error (MAPE) for ANN',horizontal=TRUE)
var(mape_nn_cv)

```







#Project Extension
The previous capstone work confirmed that machine learning can outperform ARIMA models in a linear regression time series format. However, this project will extend the applications into binary time series prediction/classification. For this project, I use the same data set.

The new proposal is to compare ARIMA with the machine learning algorithms by using the recession as a time series and the target for the algorithms.

First, I start by massaging the data set to make the recession the new target.

```{r, warning=FALSE}
#drop target
target_dropped = full_data_update[,2:length(full_data_update)]
#use recession
recession_target = target_dropped
recession_target$target = target_dropped$USRECD
recession_target = recession_target[ , -which(names(recession_target) %in% c("USRECD"))]

prediction_error = function(y,yhat){
  results = ifelse(yhat>.05,1,0)
  print(confusionMatrix(results,y))
  return(mean(results != y))
}

```




#ARIMA
First, I run ARIMA to see how well it performs in backtests in both the recursive and rolling window setting. I do not expect it to do well.

```{r, warning=FALSE}

ts_r = ts(recession_target$target)

#benchmark
recursive=(backtest(ts_r,1,"recursive"))
rolling=(backtest(ts_r,1,"rolling"))
print(prediction_error(recursive[[1]],recursive[[2]]))
print(prediction_error(rolling[[1]],rolling[[2]]))

```

The results show the opposite and show that ARIMA does an excellent job in classifying the recession. On closer inspection, however, the ARIMA model is predicting a recession tomorrow if there is a recession today and similar for non-recessions. The test set was split from the training set in the middle of a recession, therefore it starts with a string of recession days with no other recessions after this one ends. This does not give us a good method to predict oncoming recessions and only tells us we are likely to be in a recession tomorrow if we are in one today. Therefore, ARIMA fails at the task of giving a liklihood estimate and the accuracy rating is misleading.

#Machine Learning
The techniques used in this section are the same as the capstone portion and are translatable into the binary classifcation setting. Before running our machine learning algorithms, the data needs to be randomly split into 2/3 training and 1/3 test.

```{r, warning=FALSE}

#create training and test sets
## 66% of the sample size
smp_size <- floor(.66* nrow(recession_target))

## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(recession_target)), size = smp_size)

train <- recession_target[train_ind, ]
test <- recession_target[-train_ind, ]


#normalize data for nn
maxs <- apply(recession_target, 2, max) 
mins <- apply(recession_target, 2, min)
scaled <- as.data.frame(scale(recession_target, center = mins, scale = maxs - mins))
train_ <- scaled[train_ind,]
test_ <- scaled[-train_ind,]


#y_index
y_index = length(recession_target)

```

#LASSO
There are two main purposes for LASSO:

1. Prediction
2. Shrinkage

LASSO is capable of giving us a good binomial prediction, but it is also able to shrink our data set to remove irrelevant variables that will help for neural network tuning.

```{r, warning=FALSE}

#lasso
x <- model.matrix( ~ .-1, train[ , -y_index])
y <- data.matrix(train[, y_index])

model.lasso <- cv.glmnet(x, y, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE)
plot(model.lasso)
plot(model.lasso$glmnet.fit, xvar="lambda", label=TRUE)
model.lasso$lambda.min
model.lasso$lambda.1se
coef(model.lasso, s=model.lasso$lambda.min)

test_x = test[,-1]
test_x_matrix = model.matrix( ~ .-1, test[,-y_index])
lasso_test = predict(model.lasso, newx=test_x_matrix,type="link")


lasso_error = prediction_error(test$target, lasso_test)
print(lasso_error)

```

We can see the results from LASSO have a high accuracy. However, the accuracy is misleading as it was in ARIMA because there is a disproportionate ammount of 0s compared to 1s in the data set. It is important to look at the confusion matrix. We can see that LASSO actually misclassified 1s a lot. It wrongly estimated a 0 155 times and wrongly estimated a 1 40 times and only correctly estimated a 1 74 times. This is still better than ARIMA for giving us a liklihood estimate, but it is not strong enough.

#Decision Trees
We use random forests so we don't have correlated trees or high variance from sample.

```{r, warning=FALSE}

#decision tree
tree_fit <- randomForest(target ~ .,   data=train)
print(tree_fit) # view results 
importance(tree_fit) # importance of each predictor
varImpPlot(tree_fit, main = "Importance Plot (top 15)", n.var = 15)

test_x = test[,-y_index]
test_x_matrix = model.matrix( ~ .-1, test[,-y_index])
tree_test = predict(tree_fit, newdata=test_x)

tree_mape = mape(test$target,tree_test)
tree_mape

tree_error = prediction_error(test$target,tree_test)
print(tree_error)

```

This gives us a very good prediction accuracy, but we must remember that the prediction accuracy is deceiving. Therefore, we must look at the confusion matrix. From the confusion matrix, we see that it does not wrongly predict a 0 when the actual was a 1. It does however predict a 1 95 times when it should have been a 0. It correctly predicts a recession 229 times. This is much better than the LASSO results. Based on these results, we can take this as a cautious approach in estimating a recession prediction model.

#Neural Network
Since neural networks proved to be very powerful in the capstone section, there is a chance that they may prove extremely capable of recession prediction. I start by using the same neural network setup from the untuned capstone network as a starting point.

```{r, warning=FALSE}
n <- names(train_)
f <- as.formula(paste("target ~", paste(n[!n %in% "target"], collapse = " + ")))
set.seed(1)
nn <- neuralnet(f,data=train_,hidden=c(100,80,70,60,50,40,30,20),linear.output = FALSE)
pr.nn <- compute(nn,test_[,2:ncol(test_)])
# pr.nn
pr.nn_ <- pr.nn$net.result*(max(recession_target$target)-min(recession_target$target))+min(recession_target$target)
test.r <- (test_$target)*(max(recession_target$target)-min(recession_target$target))+min(recession_target$target)


nn_error = prediction_error(test.r,pr.nn_)
print(nn_error)

```

The results are not assuring as there is very poor accuracy all around. The network needs to be tuned.

#Nueral Network Tuning
I start by dropping variables that were shrunk from LASSO. This will help reduce noise from irrelevant variables.

```{r, warning=FALSE}
#extract non 0s from lasso
coefs = coef(model.lasso, s=model.lasso$lambda.min)

non_0_coefs=c()
for( i in 2: length(coefs) ){
  if(coefs[i]!=0){
    non_0_coefs = c(non_0_coefs,rownames(coefs)[i])
  }
}

#add in target
non_0_coefs = c("target",non_0_coefs)
rec_data_minimized = recession_target[ , which(names(recession_target) %in% non_0_coefs)]


#recreate training/test

#create training and test sets
## 66% of the sample size
smp_size <- floor(.66* nrow(rec_data_minimized))

## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(rec_data_minimized)), size = smp_size)

train <- rec_data_minimized[train_ind, ]
test <- rec_data_minimized[-train_ind, ]


#normalize data for nn
maxs <- apply(rec_data_minimized, 2, max) 
mins <- apply(rec_data_minimized, 2, min)
scaled <- as.data.frame(scale(rec_data_minimized, center = mins, scale = maxs - mins))
train_ <- scaled[train_ind,]
test_ <- scaled[-train_ind,]


#y_index
y_index = length(rec_data_minimized)

```

After shrinking the data, I select a set of hidden layers and nodes at each layer through guesswork.

```{r, warning=FALSE}

n <- names(train_)
f <- as.formula(paste("target ~", paste(n[!n %in% "target"], collapse = " + ")))
set.seed(1)
nn <- neuralnet(f,data=train_,hidden=c(5,3,1),linear.output = FALSE,act.fct = "logistic")
pr.nn <- compute(nn,test_[,2:ncol(test_)])
# pr.nn
pr.nn_ <- pr.nn$net.result*(max(recession_target$target)-min(recession_target$target))+min(recession_target$target)
test.r <- (test_$target)*(max(recession_target$target)-min(recession_target$target))+min(recession_target$target)


nn_error = prediction_error(test.r,pr.nn_)
print(nn_error)

```


It's still not doing well. To further tune the network, the hidden layer setup needs to be optimized. By running a random sample of hidden layers and nodes, we can hope to find one that fits the data better without over or underfitting. The code is commented out for computational reasons.

```{r, warning=FALSE}
# 
# nn_func=function(nodes){
#   set.seed(1)
#   nn <- neuralnet(f,data=train_,hidden=nodes)
#   pr.nn <- compute(nn,test_[,2:ncol(test_)])
#   # pr.nn
#   pr.nn_ <- pr.nn$net.result*(max(recession_target$target)-min(recession_target$target))+min(recession_target$target)
#   test.r <- (test_$target)*(max(recession_target$target)-min(recession_target$target))+min(recession_target$target)
#   MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
#   # MSE.nn
#   nn_error = prediction_error(test.r,pr.nn_)
#   return(nn_error)
# }
# 
# iterations = 10
# #randomly select values around 30, 15, 6
# for(i in 1:iterations){
# 
#   x1 <- floor(runif(1, 2, 25))
#   x2 <- floor(runif(1, 1, 16))
#   x3 <- floor(runif(1, -10, 10))
# 
#   node_list=c()
# 
#   if(x3>0){
#     node_list = c(x1,x2,x3)
#   }
#   else{
#     node_list = c(x1,x2)
#   }
#   nn_f = nn_func(node_list)
#   print(node_list)
#   print(nn_f)
# }

```

After running the random sampling of hidden layer setup, the best setup that was found is a two layer (19,3) setup.

```{r, warning=FALSE}
n <- names(train_)
f <- as.formula(paste("target ~", paste(n[!n %in% "target"], collapse = " + ")))
set.seed(1)
nn <- neuralnet(f,data=train_,hidden=c(19,3),linear.output = FALSE,act.fct = "logistic")
pr.nn <- compute(nn,test_[,2:ncol(test_)])
# pr.nn
pr.nn_ <- pr.nn$net.result*(max(recession_target$target)-min(recession_target$target))+min(recession_target$target)
test.r <- (test_$target)*(max(recession_target$target)-min(recession_target$target))+min(recession_target$target)


nn_error = prediction_error(test.r,pr.nn_)
print(nn_error)

```

After running our "best" setup, we still do not reach results as good as the previous methods. It actually has a lower accuracy rating than if we were to predict all 0s. This could be due to many reasons including, but not limitted to, over/underfitting and not enough data. The neural network does not seem fit for this data.

#Cross Validation
By running cross validation on each of our three models, we can get a better idea of how they perform. The distribution of the prediction error is also plotted.

```{r, warning=FALSE}


#normalize data

maxs <- apply(rec_data_minimized, 2, max) 
mins <- apply(rec_data_minimized, 2, min)

scaled <- as.data.frame(scale(rec_data_minimized, center = mins, scale = maxs - mins))

set.seed(34)


pred_error = NULL
k <- 5


pbar <- create_progress_bar('text')
pbar$init(k)

for(i in 1:k){
    index <- sample(1:nrow(scaled),round(0.8*nrow(scaled)))
    train.cv <- scaled[index,]
    test.cv <- scaled[-index,]

    nn <- neuralnet(f,data=train.cv,hidden=c(19,3),linear.output=FALSE)

    pr.nn <- compute(nn,test.cv[,2:ncol(test.cv)])
    pr.nn <- pr.nn$net.result*(max(rec_data_minimized$target)-min(rec_data_minimized$target))+min(rec_data_minimized$target)

    test.cv.r <- (test.cv$target)*(max(rec_data_minimized$target)-min(rec_data_minimized$target))+min(rec_data_minimized$target)



    pred_error[i] = prediction_error(test.cv.r,pr.nn)
    
    
    pbar$step()
}


mean(pred_error)


boxplot(pred_error,xlab='Prediction CV',col='cyan',
        border='blue',names='CV error (Prediction Error)',
        main='CV K-Fold (5) error (Prediction) for Neural Network',horizontal=TRUE)
var(pred_error)


```


```{r, warning=FALSE}

set.seed(34)


pred_error = NULL
k <- 5


pbar <- create_progress_bar('text')
pbar$init(k)

for(i in 1:k){
    index <- sample(1:nrow(rec_data_minimized),round(0.8*nrow(rec_data_minimized)))
    train.cv <- rec_data_minimized[index,]
    test.cv <- rec_data_minimized[-index,]
    x <- model.matrix( ~ .-1, train.cv[ , -y_index])
    y <- data.matrix(train.cv[, y_index])
    test_x = test.cv[,-1]
    test_x_matrix = model.matrix( ~ .-1, test.cv[,-y_index])
    model.lasso <- cv.glmnet(x, y, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE)
    lasso_test = predict(model.lasso, newx=test_x_matrix,type="link")
    pred_error[i] = prediction_error(test.cv$target, lasso_test)
    pbar$step()
}


mean(pred_error)



boxplot(pred_error,xlab='Prediction CV',col='cyan',
        border='blue',names='CV error (Prediction Error)',
        main='CV K-Fold (5) error (Prediction) for LASSO',horizontal=TRUE)
var(pred_error)


```


```{r, warning=FALSE}

set.seed(34)


pred_error = NULL
k <- 5


pbar <- create_progress_bar('text')
pbar$init(k)

for(i in 1:k){
    index <- sample(1:nrow(rec_data_minimized),round(0.8*nrow(rec_data_minimized)))
    train.cv <- rec_data_minimized[index,]
    test.cv <- rec_data_minimized[-index,]
    tree_fit <- randomForest(target ~ .,   data=train.cv)
    tree_test = predict(tree_fit, newdata=test.cv)
    pred_error[i] = prediction_error(test.cv$target,tree_test)
    pbar$step()
}


mean(pred_error)



boxplot(pred_error,xlab='Prediction CV',col='cyan',
        border='blue',names='CV error (Prediction Error)',
        main='CV K-Fold (5) error (Prediction) for Random Forests',horizontal=TRUE)
var(pred_error)


```



We are seeing similar results we saw in the test set. The neural network is performing very poorly. LASSO looks to be doing good, but when only considering correctly and incorrectly classifying 1 (excluding predicting 0 corrrectly), it does not perform well. Decision trees are also overclassifying 1s, but that is not a bad thing as it does not missclassify a 0 as a 1 and is taking a "cautious", as defined by being rather safe than sorry, approach.

#Comparing Machine Learning Methods
By looking at each machine learning methods' confusion matrix and their cross validation results, the decision trees with a random forest implementation performs the best. It was able to correctly classify most of the recession days and did not classify an actual recession day as being not in a recession. It did overclassify the recession, however, predicting there would be a recession day when in actuallity there was not a recession that day. This makes it a more cautious approach from the human perspective. The computer does not know that 1 is bad and 0 is good which is why we need humans to look at the confusion matrix and conclude that it is a cautious model.

#Conclusion
In conclusion, we should definately be using machine learning opposed to ARIMA for predicting an oncoming recession. ARIMA only tells us that we will be in a recession tomorrow if we are in one today and we will not be in a recession tomorrow if we are not in one today. Being economists, statisticians, and data scientists, we know that this cannot be used to predict an oncoming recession, but rather just tells us what we already know. This is why machine learning is very powerful. It can give us a probability that there is a recession regardless of the previous day's recession status. 

Through the different machine learning methods, it was determined that random forests were the best for classifying a recession. The confusion matricies showed how accurate it actually was in correctly predicting a recession. After combining the results with the economic domain knowledge, the random forest model is considered a "cautious" model that it incorrectly classifies days as a recession when it actually was a non-recession day, but does not incorrectly mark days a non-recessionary days when they actually were. The importance plot from the random forest model shows that the biggest determinants for a recession is the presidential approval and dissaproval ratings.

Combing these results with the results from my capstone project, the final conclusion is that ARIMA is not as powerful as machine learning and my prediction is that ARIMA will one day be phased out and surpassed completely by machine learning.


